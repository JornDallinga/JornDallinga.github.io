---
title: "Proba-V"
author: "Jorn Dallinga, PROBA-V package used from Johannes Eberenz"
date: "`r format(Sys.time(), '%d %B, %Y')`"
Version: "1.0 (updated 6-11)"
output:
  knitrBootstrap::bootstrap_document:
    title: "ProbaV" 
    theme: cosmo
    menu: FALSE
---

#WUR PROBA-V workflow ![WUR logo](http://www.wageningenur.nl/upload/f9a0b5f2-15c5-4b84-9e14-20ef02f5e265_wur-logo.png)

```{r, echo=FALSE, message=FALSE}
library(knitr)
opts_chunk$set(fig.width=5, fig.align='center', fig.height=5, dpi=72)
# set the knitR working directory to the same file location as getwd(), in order to maintain the link to R source scripts.
knitr::opts_knit$set(root.dir = getwd())
```

## Introduction

the PROBA-V instrument

ESA is currently preparing the launch of the Sentinel-3 satellites, of which the first one (Sentinel-3A) is foreseen for September 29tn, 2015. The Sentinel missions are developed for the operational needs in land, ocean, and atmospheric monitoring within the European Copernicus programme. However, between the end-of-life of SPOT-VGT and the launch of the Sentinel-3 satellites, a rather large time gap of about 3 years would occur, which would imply a discontinuation of the vegetation monitoring time series.

In order to preserve this observational continuity, Belgium decided to build a small satellite mission based on the successful ESA PROBA expertise, using state-of-the-art-technology. PROBA-V (with "V" standing for Vegetation) was designed by a full Belgian consortium, fulfills all of the vegetation users specifications and is complement to the Sentinel-3 satellites to be launched after PROBA-V.

PROBA-V has a constellation of 3 cameras that daily observe the land surface and vegetation at similar spectral wavelengths (BLUE, RED, NIR, and SWIR) as SPOT-VGT, but with an improved spatial resolution (300 m and 100 m for the centre camera). These observations are processed into daily and 10-daily syntheses products,  available at 100 m, 300 m, and 1 km. The products can be downloaded from [vito](www.vito-eodata.be). [source quoted directly](http://proba-v.vgt.vito.be/) 

## Preperation

This script has been tested and runs on a LINUX Centos machine located on the ESA cloud toolbox, but might require some tweaking if used on other OS or versions. Most functions are designed to run parallel on multi-core machines, such as the cloud toolbox. Al though this example script uses a small case study throughout the tutorial, when extending the functions to larger areas, system time could become significantly slower (depending on how many cores you have to your disposal).

System on which the script has been tested:
```{r, echo=F, results= 'hold'}
Sys.info()['sysname']
.Platform$OS.type
version$os
```

The data used within these scripts can be downloaded from the [vito](http://proba-v.vgt.vito.be/) website. Lets start with loading the required libraries and packages to illustrate a workflow on working with the PROBA-V data. Additionally, we have to set the library path if this has not been set previously. When working from the cloud toolbox, errors could occur with loading packages when the library path has not been added.

```{r, message=F, warning=F}
# add additional libary path
.libPaths( c( .libPaths(), "~/R/x86_64-redhat-linux-gnu-library/3.2") )

## libaries
library(ranger)
library(raster)
library(ggvis)
library(rgdal)
library(dplyr)
library(devtools)
library(gdalUtils)
library(probaV)
library(tools)
library(parallel)
library(zoo)
library(RCurl)
library(stringr)
library(rasterVis)
```


The Proba-V package used can be viewed [here](https://github.com/johanez/probaV). Many fixes, including spelling, and further updating of the package is still underway. Therefore, we download and source the fixed scripts directly using the following 'source_ttps' function from another github repository. 

```{r,echo=T, results='hide'}
source_https <- function(url, ...) {
  # load package
  require(RCurl)
 
  # parse and evaluate each .R script
  sapply(c(url, ...), function(u) {
    eval(parse(text = getURL(u, followlocation = TRUE, cainfo = system.file("CurlSSL", "cacert.pem", package = "RCurl"))), envir = .GlobalEnv)
  })
}
source_https("https://raw.githubusercontent.com/JornDallinga/ProbaV_JD/master/R/cloud_filter.R")
```

## Data download

This example script uses data existing on the cloud toolbox and follows an existing directory structure. Downloading ProvaV data will not create that directory structure yet. Thus, this script will work when following the ProbaV directory structure on the cloud toolbox, but not yet with downloading the data manually to a certain directory. But if you would like to know how to download the ProbaV data, below follows an example.
Lets download the PROBA-V tiles by using coordinates! 

```{r}
# Select the location for which you wish to download the PROBA-V  tiles for. 
x <- 10.00 # Longitude
y <- 8.00 # Latitude

# In order to download the correct tile numbers, we use coordinates to convert to the tile number
tile <- probaVTileFromCoords(x, y)
tile_str <- paste("\'*",tile,"*\'", sep ="")

```

To download data from the PROBA-V vito website, we need to be a registerd user (free). If you dont allready have an account, feel free to make one by following this [link](http://www.vito-eodata.be/PDF/portal/Application.html#Home). Assign the username and password to a variable, which you can enter below.

```{r, results='hide'}
u_name <- readline("Type the username:")
p_word <- readline("Type the password:")
```
```{r}
# enter the file directory for the download location
dirloc <- paste(getwd(), '/2015/', sep = "")
# Create file directory if it does not exist yet
dir.create(file.path(dirloc), showWarnings = FALSE)
```

Use wget to download the data. wget has be passed directly in the terminal. Therefore, we pass the string to the system command. In this example, the link in the string will have to be adjusted manually to your request. The dates at the end of the http adres below, allow you to download for a specific year, month and/or day. If you wish to download all ProbaV data for a specific year (e.g. 2016), then change the dates to '2016/?/?mode=tif'. The data will be downloaded into the directory set in the previous code (dirloc). For more examples on how to download ProbaV data on different operating systems, please follow this [link](http://www.vito-eodata.be/PDF/image/Data_pool_manual.pdf).

```{r}
wget_string <- paste('wget -A ', tile_str ,' .tif' , ' -P ', dirloc, ' --no-directories -r --user=', u_name,' --password=', p_word, ' http://www.vito-eodata.be/PDF/datapool/Free_Data/PROBA-V_100m/S5_TOC_100_m/2016/2/1/?mode=tif', sep="")

# check for output of the download string
print(wget_string)
```
```{r, eval=FALSE}
# pass the string to the terminal directly from R.
system(wget_string)
```

## Preprocessing

Lets start with the preprocessing of the data. Remember, the following parts of the script only work by following the ProbaV directory structure on the cloud toolbox. Provided by Vito or you can download manually, but the files should be stored in the folder structure used by vito (One folder per date, in YYYYMMDD format, contains files for all tiles).

The PROBA-V has 3 synthesis products, S1 (daily observations), S5 (mosaiced 5 day observation) and S10 (mosaiced 10 day observation). In the case of a 10-day synthesis product, the product has a minimum effect of cloud cover, resulting from selection of cloud-free acquisitions during the 10-day period. Additionally, their are two products amongst these daily products available, the Top of Atmosphere (TOA) and Top of Canopy (TOC). The TOC is atmospheric corrected product, and in combination with the S5 synthesis, is the data we are going to use in this tutorial.

Below the data location of my PROABA_V data (l0_dir), and where I intend to store my new datasets (outdir)

```{r, echo=F}
# set your data path in which you have the .tiff files of ProbaV data
l0_dir <- "/data/MTDA/TIFFDERIVED/PROBAV_L3_S5_TOC_100M"
# set data path to which you would write large files
outdir <- "/userdata/sm2"
```

start by setting our data path which contains the downloaded files. The following section loops through the downloaded data folder and retrieves the product information, such as tiles and acquisition dates. Since the we only work with .tif files (for now), we add '.tif$' to the pattern selection.

```{r, eval=F}
# variable setting for data path
10_dir <- "set data path" 
# if you manually downloaded the data
10_dir <- dirloc
# set data path to which you would write large files
outdir <- outdir
```

The files should be stored in the folder structure used by vito, using YYYYMMDD notation for the folders that contain the .tif files. Where one folder per date (YYYYMMDD) contains files for all tiles. Through which folders you wish to loop the product information function, is up to you. For this tutorial, we select a preset of folders based on subfolder directory lenght. In this scenario, I know that the YYYYMMDD folder notations contain the correct .tif files. Since there are several folders that still contain unprocessed PROBAV data in my 'l0_dir' path, I want to exclude them. 

NOTE: If you wish to simply use one folder name as directory(l0_dir), note that all the .tif files in the subfolders will be selected in the looping process, including folders that might contain incorrect/unprocessed .tif files.

```{r}
# list files in the main directory that stores the YYYYMMDD folders containing the PROBAV data
lf <- list.files(l0_dir)
# Simple selection of folders based on name length == YYYYMMDD
lf <- lf[nchar(lf) == 8]
# Create coerced subfolder variable which contain all the PROBAV .tif files that you wish to process.
dir_file <- paste0(l0_dir,'/',lf)
```

The dir_file variable now contains all the folders in which we would look for the ProbaV .tif files. The structure of this variable is shown below:

```{r}
class(dir_file)
head(dir_file)
```

Now lets loop through this directory (dir_file) and collect all the metadata for a specific tile. The selection of dates will be done later.

```{r}

# create a loop to apply the getProbaVinfo function on a group of folders
datalist = list()
tiles <- 'X17Y06'
for (i in 1:length(dir_file)){
  dat <- getProbaVinfo(dir_file[i], pattern = ".tif$", tiles = tiles)
  datalist[[i]] <- dat # add it to your list
}
# merge the created dataframes into a single dataframe of product information on your selected tiles.
df_probav_down <- do.call(rbind, datalist)
```

Below a simple figure on the number of .tif's per band that we retrieved for a our selected tile in the dir_file directory. 

```{r}
# plot the retrieved data
df_probav_down %>% ggvis(x=~tile, fill=~band) %>% layer_bars()
```

## clean data 
The function 'getProbaVQClist' creates a dataframe of state mask (SM) codes based on the quality indicators in the Status Map dataset. It provides an overview of clear codes per channel. Explanation on these SM codes can be found in the [ProbaV Product User Manual](http://proba-v.vgt.vito.be/sites/default/files/PROBAV-Products_User_Manual_v1.2.pdf), which can also be downloaded directly from the Vito [website](http://www.vito-eodata.be/PDF/portal/Application.html#Home).

For clear pixel selection, we use the SM codes with 'clear_all', which means that a certain pixel is not marked as shadow, cloud, ice or sea. Additionally, it selects only the 'good' quality pixels of the radiometric bands (SWIR, NIR, RED, BLUE).

```{r}
# Top of dataframe showing SM codes. 
head(getProbaVQClist()$all_bits)
# Select quality control value with clear channels
QC_val <- getProbaVQClist()$clear_all
# The QC_val now stores the high quality value:
print(paste('The Quality control value: ',QC_val), sep = "")
```

The Radiometry brick contains 4 raster layers. Lets split the radiometry brick into single .tif layers (SWIR, NIR, RED, BLUE). You can select which tiles you would like to process. In this workflow, we are going to work with 3 bands, the Blue, SWIR and NDVI bands. However, the function will extract all the bands that the Radiometry brick contains, including the Red and NIR band. This allows you to play around with more band selections.

```{r}
# First, select the radiometry .tif files from the download folder and which tiles to use
patterns <- c('RADIOMETRY.tif$') # "NDVI.tif$"
tiles <- c("X17Y06") # c("X16Y06","X11Y07", "X11Y08", "X11Y09", ...)

# Lets gather the information from the radiometry band using the variables assigned above. 
df = list()
for (i in 1:length(dir_file)){
  dat <- getProbaVinfo(dir_file[i], pattern = patterns, tiles = tiles)
  df[[i]] <- dat # add it to your list
}

df <- do.call(rbind, df)
```

Lets have a 'glimpse of our data'. The glimpse function shows the metadata of our dataframe created from the 'getProbaVinfo' function.

```{r}
# lets get a glimpse of the currently available data in our selection of patterns and tiles.
glimpse(df)

#How many radiometry bands do we have of the assigned tiles?
print(paste(nrow(df), " radiometry bands", sep = ""))
```

Next step is to keep the quality control values in the data which correspond to the QC_val as result of the 'getProbaVQClist' function. Lets test this on a single date/location. Select a RADIOMETRY.tif or NDVI.tif file from the downloaded data folder and assign it to a variable. 

```{r}
# Assign variable from the download location of your Geotiffs. Lets use the NDVI.
r <- paste(l0_dir, "/20160116/PROBAV_S5_TOC_20160116_100M_V001/PROBAV_S5_TOC_X17Y06_20160116_100M_V001_NDVI.tif", sep = "")

# Assign a file.path (including filename) to which we will write the cleaned data.
filename <- paste(outdir,"/PROBAV_S5_TOC_X17Y06_20160116_100M_V001_NDVI_sm.tif", sep = "")
```
```{r, eval=F}
cleanProbaV(f_data = r, filename=filename, QC_val = QC_val, fill=255, datatype="FLT4S", as.is = F, overwrite = F)
```

The result is a cleaned raster dataset (right) with only the quality control value (QC_val) selected.

```{r}
par(mfrow = c(1, 2))
plot(raster(r))
plot(raster(filename))
par(mfrow = c(1, 1))
```


## Proccessing a ProbaV bath
The processProbaVbatch function uses the cleanProbaV function on a whole 'batch' of data. The starting date parameter helps selecting at which starting point you wish clean the data. The function uses parallel processing, which is useful when the machine has multiple processors or/and cores. The processing time of the following functions thus depend on the process capacity of your machine. In order to process both the Radiometry and the NDVI datasets, we will have to run the processProbaVbath function twice, with different patterns.

```{r, eval=F}
# How many cores do we have:
detectCores(all.tests = FALSE, logical = TRUE)

# Select the patterns to match the .tif files you wish to process.
patterns <- c('RADIOMETRY.tif$') # or "NDVI.tif$"

# similar for NDVI
processProbaVbatch(dir_file, 
                    pattern = patterns, tiles = tiles, start_date = "2015-10-25", end_date = NULL,
                    QC_val = QC_val, outdir = outdir,
                    ncores = (detectCores(all.tests = FALSE, logical = TRUE)-1),
                    overwrite=F)
```

## Input check for next functions

The following section sets the required parameters for the next function, the 'virtual raster stack'. Most of the parameters are pretty basic, or are repeated here as reminder. 

```{r}
# ----- check input --- #
tn <- 1 # tile number (tn) 
# Select tiles of your study area. Multiple tiles can be coerced together. 
tiles <- c("X17Y06") # c("X18Y02", "X18Y03")
```

Lets use the getProbaVinfo function again to visualize a certain tile data that we have.

```{r}
# set directory of the processed files as a result of the processProbaVbatch function
probav_sm_dir <- paste(outdir,"/", sep = "")
# lets retrieve information on the available data that we have selected by setting our parameters.
df_probav_sm <- getProbaVinfo(probav_sm_dir, pattern =  '_sm.tif$', tiles = tiles)
# visualize product information, such as bands and acquisition dates. 
glimpse(df_probav_sm)
```


```{r}
# ----- next parameters ----#
# read and assign the unique available bands to a variable
bands <-  df_probav_sm[df_probav_sm$date == df_probav_sm$date[1], 'band']
#unique(df_probav_sm$band)
# read and assign the acquisition dates to a variable
dates <-  df_probav_sm[df_probav_sm$band == bands[1], 'date']
# minrows is a parameter for the coming raster processing functions. It allows you to set the minimal rows that the coming calc functions will apply their algorithm on simultaneously.
minrows = 15
# check for available cores and set cores (minus 1)
mc.cores = detectCores(all.tests = FALSE, logical = TRUE)-1
# create directory for the log files
dir.create("rsdata", showWarnings = FALSE)
dir.create("rsdata/probav", showWarnings = FALSE)
dir.create("rsdata/probav/logs", showWarnings = FALSE)
# set log name
logfile <- file.path(getwd(), paste0("rsdata/probav/logs/", tiles, ".log"))
# create directory for output results and temp raster results
dir.create("rsdata/probav/metrics", showWarnings = FALSE)
dir.create("/userdata/temp", showWarnings = FALSE)

rasterOptions(todisk = F,
              tmpdir = file.path("/userdata/temp", collapse =""))
```

Select bands:
Band selection for the the stacking of raster layers. For this tutorial, we use the BlUE, SWIR and NDVI bands for the classification. However, you can select any of the available bands from the PROBA-V sensor (e.g. including NIR/RED).

```{r}
# Select the bands to use in sequential functions
bands_select <- '(NDVI|SWIR|BLUE|RED0)' # e.g. '(BLUE|SWIR|NDVI)' or '(BLUE|SWIR)' or 'NDVI'
#bands_select <- 'NDVI' # e.g. '(BLUE|SWIR|NDVI)' or '(BLUE|SWIR)' or 'NDVI'
# Include the .tif extension 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")
```

## virtual stack
Create a virtual stack from selected bands. The virtual raster stacking is an alternative to the more conventional raster layer stacking through [stack](https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/stack) or [brick](https://www.rdocumentation.org/packages/raster/versions/2.5-8/topics/brick). Virtual raster stacking is an R wrapper for the 'gdalbuildvrt' function that is part of the Geospatial Data Abstraction Library (GDAL). Opposed to raster::stack or raster::brick, the virtual rasterstack function creates a file (.vrt), linked to the filepath environment of selected datasets. Major benefit is that GDAL vrt was found to query faster than raster::stack.

```{r}
# --- build a vrt ---#
# check the gdal version installed on the machine
gdalinfo(version = T)

# create virtual raster stack output name
vrt_name <- file.path(outdir, paste0(tiles, "_",paste0(bands, collapse = "_"), ".vrt"))
```

The virtual raster function within the Proba-V package can create two outputs, a virtual raster stack, or a dataframe including the metadata of the included rasters. This preference can be set by the 'return_raster' variable. Other parameters within the function are the output name (vrt name). the tiles to stack and the available start and end date of the tile selection. We are selecting the whole availabe range of dates.

As can be observed from the dataframe output (df_probav_sm), we get to retrieve the metadata of our selection. 

```{r}
b_vrt <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles[1], return_raster = T, start_date = "2015-10-21", end_date = "2015-10-21")
  
df_probav_sm <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles[1], return_raster = F, start_date ="2015-10-21", end_date = "2015-10-21")
```

Lets see what our dataframe contains. The output of the virtual raster stack which returns as a dataframe is a collection of relevant product information within the limits of your date selection and tile selection.

```{r}
# returning first four rows of the created dataframe.
head(df_probav_sm, n=4)
```

Lets plot the first 4 individual bands from the first date from our virtual raster stack.

```{r}
b_vrt_s <- subset(b_vrt,1:4)
plot(b_vrt_s)
mtext("Plotting the 4 bands", outer=F,  cex=1, line=-0.5)
```

## cloudfilter example

"Clouds in satellite observations obstruct the retrieval of vegetation parameters. Therefore a proper cloud screening is pivotal in the pre-processing of the various value-added products. The PROBA-V cloud detection algorithm is a modified version of the method applied to the SPOTVGT
BLUE and SWIR observations (Lissens et al., 2000). Using these band reflectances, two
separate cloud masks are created. The first (second) mask uses a BLUE (SWIR) band threshold with
an additional check at 300 m resolution on the SWIR (BLUE) band. The final cloud mask is a merge
of these two masking results. Compared to the SPOT-VGT cloud mask, some modifications were
necessary, because the assumption that clouds are observed at the same position in both the BLUE
and SWIR bands is no longer valid for PROBA-V, due to the observation time difference."  [source](http://proba-v.vgt.vito.be/sites/default/files/Product_User_Manual.pdf) p.17. 

In addition to the added cloud/shadow algorythm applied on the Proba-V products, the Proba-V R package adds an additional cloud filter which requires some mannual interpretation. The following section will visualize the difference between the cloud/shadow algorytm from Proba-V and the created cloud filter function in the Proba-V R package.

We will start of with a false color image of our subset area previous defined by the extent function. We will simply use all bands available. In order to create a false color image, we will have to rerun our processbatchfunction without filtering the cloud/shadow pixels.

```{r}
# Select all the bands for a false color image from the RADIOMETRY stack
bands_select <- '(BLUE|SWIR|NIR0|RED0)' 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")

# run process batch on the unfiltered data
# Here we change the QC_value to select all the cells, including the cloud values.
# Write the clouds to a new directory, so it wont mess up the original file
# Select one point in time, lets say: 2015-04-16

cloud_dir <- "rsdata/probav/sm_Withclouds"
dir.create(file.path(cloud_dir), showWarnings = FALSE)

# select dir with clouds
probav_cloud_dir <- paste(getwd(),'/',cloud_dir, sep = "")
l0_dir1 <- "/data/MTDA/TIFFDERIVED/PROBAV_L3_S5_TOC_100M/20151021"
```
```{r, eval=F}
# we wont use NDVI for the cloud filter in this example, just the radiometry is needed
patterns <- c('RADIOMETRY.tif$')
processProbaVbatch(l0_dir1, 
                    pattern = patterns, tiles = tiles, start_date = "2015-10-21", end_date = "2015-10-21",
                    QC_val = 1:255, outdir = probav_cloud_dir,
                    ncores = 5,
                    overwrite=F)
```

Rerun the virtual stack function on our newly created directory with unfiltered data. Since we are working with a specific start and end date to visualize a false color image on a fixed time, we adjust our start and end date accordingly. Set a smaller extent for this tutorial. The smaller the extent (region of interest), the faster the processing. We set the following extent by assigning the x and y coordinates accordingly (long lat, WGS84)

```{r}
# set extent
xmin <- -2.064549  
xmax <- -1.074974 
ymin <- 8.840184
ymax <- 10.39336

# select directory with clouds
probav_cloud_dir <- paste(getwd(),'/',cloud_dir,'/', sep = "")
# create virtual raster output name
vrt_name <- file.path(outdir, paste0(tiles, "_",paste0(bands, collapse = "_"), ".vrt"))
# load the image in our virtual raster stack
b_vrt_cloud <- timeVrtProbaV(probav_cloud_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles, return_raster = T, start_date = "2015-10-21", end_date = "2015-10-21", te = c(xmin,ymin,xmax,ymax))

# plot a false color image
plotRGB(b_vrt_cloud, 3, 2, 1, stretch='lin')
```

Well that looks cloudy! Good, now we can start adding the Proba-V cloud mask and the Proba-V R package cloud filter. Lets start with visualizing the cloud detection method inbedded in the Proba-V data.

```{r}
# cloud mask SM
bands_select <- '(BLUE|SWIR|NIR0)' 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")

# create virtual raster stack output name
vrt_name <- file.path(paste0(outdir,"/",tiles, "_",paste0(bands_select, collapse = "_"), ".vrt"))

# load the image in our virtual raster stack
b_vrt_cloud_mask <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles, return_raster = T, start_date = "2015-10-21", end_date = "2015-10-21", te = c(xmin, ymin, xmax, ymax))

# assign virtual raster stack to new variable
r <- b_vrt_cloud_mask
```

all NA values in our raster(r) are in this scene cloud pixels. We assign a value (9999) to these cloud pixels in order to visualize them against our cloud filter function later on. 

```{r}
b_vrt_cloud_mask[is.na(r)] <- 9999
b_vrt_cloud_mask[b_vrt_cloud_mask < 9999] <- NA
b_vrt_cloud_mask <- subset(b_vrt_cloud_mask,1)
```

Lets plot the original cloud detection method inbedded within the ProbaV data.

```{r}
par(mfrow=c(1,2))
plotRGB(b_vrt_cloud, 3, 2, 1, stretch='lin')
plotRGB(b_vrt_cloud, 3, 2, 1, stretch='lin')
plot(b_vrt_cloud_mask, col = 'orange', legend = F, add = T)
par(mfrow=c(1,1))
```

Looks good! The orange area here visualises the clouds/shadow and bad pixels detected by the cloud detection algorythm within Proba-V. If you look closely, you can still observe light cloud and shadow remnants, mostly near the orange masked clouds. Altough this original cloud detection algorythm allready masked out almost all clouds, we would like to clear as many clouds (and shadow) as possible, so we have a clear reflectance of the pixels.

Lets apply the additional cloud filter! In this example we use the blue band to detect additional cloud and shadow remnants in the landscape.

```{r}
bands_select <- '(BLUE)' 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")
vrt_name <- file.path(paste0(outdir,"/",tiles, "_",paste0(bands_select, collapse = "_"), ".vrt"))
# Select all dates of the blue band
b_vrt <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles, return_raster = T, start_date = "2015-10-21", end_date = "2016-03-01", te = c(xmin, ymin, xmax, ymax))

dir.create('/userdata/cloud_filter')
out_name <- paste('/userdata/cloud_filter/cloud_filter.envi', sep = "")

```
```{r, eval=F}
bands_select <- '(BLUE)' 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")

cloud_filter(x = b_vrt, probav_sm_dir = probav_sm_dir, pattern = bands_sel,
                          tiles = tiles, minrows = minrows, mc.cores = mc.cores,
                          logfile=logfile, overwrite=T, span=0.3, 
                          cf_bands = c(1), thresholds=c(-80, Inf), 
                          filename = out_name)

blue_c_filter <- brick(out_name)
```

Lets select the same data as our false colour image from the raster brick we just created. 

```{r, eval=F}
# select date
blue_c_filter <- subset(blue_c_filter ,1)
```

The cloud_filter function returns the Quality Control values. Meaning: "QC" is the status of observations: 0=Missing/band input QC, 1=good, 2=temporal outlier. Here we assign new values to our raster to visualise what the SmoothLoes function detects by the extra clouds

```{r, eval=F}
# copy the data set
blue_c_filter_copy <- blue_c_filter
# assign values
blue_c_filter[blue_c_filter_copy != 2] <- NA 
blue_c_filter[blue_c_filter_copy == 2] <- 1
```
```{r, results='hide'}
blue_c_filter <- brick(out_name)
blue_c_filter <- subset(blue_c_filter ,1)
# copy the data set
blue_c_filter_copy <- blue_c_filter
# assign values
blue_c_filter[blue_c_filter_copy != 2] <- NA 
blue_c_filter[blue_c_filter_copy == 2] <- 1
```

Allright! Now lets plot our cloud filter!

```{r}
plot(blue_c_filter, col = 'darkred', legend = F)
```

Lets plots the original false colour again against the 2 cloud filters

```{r}
# data copy
r3 <- b_vrt_cloud_mask
# assign pixel values 
r3[blue_c_filter == 1] <- 1
# set breakpoints
breakpoints <- c(0,2,9999)
# Assign colors to breakpoints
colors <- c("darkred","orange")
# Set layout
par(mfrow=c(1,3), layout(matrix(1:6, 1,1, byrow = TRUE)))
# Plotting
plotRGB(b_vrt_cloud, 3, 2, 1, stretch='lin')
plotRGB(b_vrt_cloud, 3, 2, 1, stretch='lin')
plot(b_vrt_cloud_mask, col = 'orange', legend = F, add = T)
plotRGB(b_vrt_cloud, 3, 2, 1, stretch='lin')
plot(r3,breaks=breakpoints,col=colors, add = T, legend = F)
# Reset layout
par(mfrow=c(1,1))
```

The cloud detection algorythm inbedded in the ProbaV data detects most of the clouds. Our additional cloud filter marks some clouds/shadow fragments missed by the cloud detection algorythm. As can be observed, these missed cloud pixels are mainly distributed near the edges of existing clouds.


## Time series

Lets start of with creating a small time-series subset of a single pixel, randomly chosen from a raster stack. Since we are interested in the seasonal trends of vegetation, lets select the NDVI band for the following example. First step is to create a virtual raster stack from a selected time range on our region of interest (extent).

```{r,eval=T}
bands_select <- '(NDVI)' 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")
vrt_name <- file.path(paste0(outdir,"/",tiles, "_",paste0(bands_select, collapse = "_"), ".vrt"))
# Select all dates of the blue band
b_vrt <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles, return_raster = T, start_date = "2014-01-21", end_date = "2016-09-01", te = c(xmin, ymin, xmax, ymax))

# Select random pixel of our raster stack
z <- zoo(c(b_vrt[runif(1, 0, ncell(b_vrt))]), getZ(b_vrt))

#unique(df_probav_sm$band)
# read and assign the acquisition dates to a variable
df_probav_sm <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles[1], return_raster = F, start_date ="2014-01-21", end_date = "2016-09-01")

bands <-  df_probav_sm[df_probav_sm$date == df_probav_sm$date[1], 'band']
dates <-  df_probav_sm[df_probav_sm$band == bands[1], 'date']

```

Lets apply the smoothLoess function. This function requires a time series [ts](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/ts.html) object. We created one using the [zoo](https://cran.r-project.org/web/packages/zoo/index.html) package. The smoothLoess function uses local polynomial regression fitting, by using the [loess](https://stat.ethz.ch/R-manual/R-devel/library/stats/html/loess.html) function. The smoothLoess function requires the dates to match with the index values, hence we retrieved the matching dates above. The threshold parameter allows you to set the upper and lower threshold for non-outlier distances from the smoothed time series. In other words, it masks certain values as outliers based on your defined threshold. We used this function before in the cloudfilter section of this tutorial. The span parameter controls the degree of smoothing. The higher the span value, the higher the smoothing of a time series object. 

```{r}
# use smoothloes to create time series
# We excluded the threshold parameter and simply plot the output of the smoothLoess function. 
f <- smoothLoess(tsx = z, QC_good=NULL, dates=dates , res_type=c("all"), span=0.3)
plot(f, main = "plotting time series.
     x = NDVI values for a given time,
     QC_good = Quality value, where 0 is missing and 1 is available,
     filled = the result of smoothing from the polynomial regression loess model", xlab = "Dates index")
# Observe the difference between span widths of the degree of smoothing.
f <- smoothLoess(tsx = z, QC_good=NULL, dates=dates , res_type=c("all"), span=0.75)
plot(f, main = "plotting time series.
     x = NDVI values for a given time,
     QC_good = Quality value, where 0 is missing and 1 is available,
     filled = the result of smoothing from the polynomial regression loess model", xlab = "Dates index")
```

## get harmonic metrics

We use a seasonal model approach and descriptive statistics. Per band and pixel i we derive median and percentiles and fit a liner harmonic model to the time series.
The coefficients of this model are then used as metrics for overall level and seasonality. The number of output coefficients depend on the parameter 'order'. QC-good parameter ensures that values that are missing (0 or not 1) are omitted. Also values that are marked as additional clouds by the smoothLoess function (value 2 in QC-good) will thus be ignored by the linear model.

```{r,eval=T}
d <- getHarmMetrics(f$x,QC_good = f$QC_good ,dates = dates, sig = .95, order = 1)
d
```

## Run metrics with a raster brick output

The following function runs the getHarmMetrics to our virtual raster brick and returns the model coefficients. Each coefficient is thus returned as a raster layer, where the output number of rasterlayers depend on the 'order' that you assigns. Hence the function

Lets start of with setting the required variables for the function parameters. 
```{r, eval=T}
# create output name on the metrics
out_name <- file.path(getwd(), paste0("rsdata/probav/metrics/",tiles[tn],"_harm_lm2_loess_03_scaled.envi"))

bands_select <- '(BLUE|SWIR|NDVI)' 
bands_sel <- paste(bands_select,'_sm.tif$', sep = "")
vrt_name <- file.path(paste0(outdir,"/",tiles, "_",paste0(bands_select, collapse = "_"), ".vrt"))

# Create virtual stack
b_vrt <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles, return_raster = T, start_date = "2014-01-21", end_date = "2016-09-01", te = c(xmin, ymin, xmax, ymax))

df_probav_sm <- timeVrtProbaV(probav_sm_dir, pattern = bands_sel, vrt_name = vrt_name, tile = tiles[1], return_raster = F, start_date ="2014-01-21", end_date = "2016-09-01")

bands <-  df_probav_sm[df_probav_sm$date == df_probav_sm$date[1], 'band']
dates <-  df_probav_sm[df_probav_sm$band == bands[1], 'date']
```

Check layers, bands, blocks and cores! Each block represents the number of rows that the 'getHarmMetricsSpatial' function applies to. This speeds up the calculation time significantly. As normally the linear model of getHarmMetrics would be calculated to each cell of a rasterbrick, it now applies the function simultaneously to 15 (or n minrows) rows in parallel. 

```{r,eval=T}
# --- get metrics ---  #
cat(sprintf("\nlayers: %i  | bands: %s  | blocks: %i  | cores: %i\n",
            nrow(df_probav_sm), paste0(bands, collapse = " "),
            blockSize(b_vrt, minrows = minrows)$n, mc.cores))
```

Now lets try out the getHarmMetricsSpatial function! Most of the functions parameters must be familiar by now. The cf-bands (cloud_free bands) is the coerced selection of the bands to which you would like to apply the cloud filter. My 'bands-select' variable is set to 3 bands, the BLUE, SWIR and NDVI. By setting the cf-bands to c(1,2), I apply the additional cloud filter to band 1 and 2 from my selected bands, in this scenario the BLUE and SWIR. The thresholds follow a similar process, where the given thresholds below apply to the first 2 bands. Each 2 values represent the min and max of your selected threshold, which are the min/max values to detect addtional clouds/shadow. 

The getHarmMetricsSpatial is basicly an 'all in one' function of the previous sections, applying the cloud filter to a region of interest using your selected band. Depending on your selected region of interest in the timeVrtProbaV2 function and your available cores/memory, this could take some time! So feel free to grab something to eat or a coffee.

```{r, eval=F, results= 'hide'}
b_metrics <- getHarmMetricsSpatial(x = b_vrt, minrows = minrows, mc.cores = mc.cores,
                                      logfile=logfile,
                                      overwrite=T, span=0.3,
                                      cf_bands = c(1,2), thresholds=c(-80, Inf, -120, 120),
                                      filename = out_name, df_probav_sm = df_probav_sm, 
                                      order = 1, datatype="INT2S", scale_f = c(10,100,10))
```

## plot the metrics

```{r,eval=T}
# metrics info
b_metrics <- brick(out_name)

# band order
# names c("min", "max", "intercept", names(lmh$coefficients)[-1])

# plotting metrics
sub_plot <- subset(b_metrics, 1:6)
plot(sub_plot)
```

## Classification process

Lets classify! Prepare classification data

---------------------------------------SCRIPT UPDATE, DONT RUN THIS SECTION----------------------------------

```{r,eval=T}
csv <- getURL("https://raw.githubusercontent.com/JornDallinga/JornDallinga.github.io/master/ref/ref_JD.csv")
pts <- read.csv(text = csv)

#save(pts, file = 'ref/ref_JD')
#ptss <-load('ref/ref_JD')

#write.csv(pts, "ref/ref_JD.csv")
#pnt_JD <- readOGR(file.path(getwd(), "ref"), "ref_Glc2ViirsStepNmoGeowikiGlob_point_africa", stringsAsFactors = F)
# crop reference data to metrics extent

#pts <- read.csv("ref/ref_JD.csv")
# to dataframe
#pts=as.data.frame(pnt_JD)
#pts <- pts[,c("ID_1","Y","X","G9_cl1")]
# fix coordinates error
coordinates(pts) <- ~X+Y
# add projection
projection(pts) <- '+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0'
# copy dataset
pnt_JD <- pts
# crop dataset
pnt_JD <- crop(pnt_JD, b_metrics)
# plot an image with the point reference data.
plot(b_metrics$Band.1)
plot(pnt_JD, add = T, col = 'red')
```

Extract cell value per reference point and create a merged dataframe

```{r,eval=T}
# ----------------------- extract per tile ------------------#
tiles <- c("X17Y06") # ..., "X17Y06", "X18Y06", "X19Y06")

# run parallel processing on tiles
registerDoParallel(min(4, length(tiles)))
df_covs_JD <- foreach(tile=tiles, .combine=rbind, .inorder = T) %dopar% {
  print(tile)
  b_metrics <- brick(paste0(getwd(), "/rsdata/probav/metrics/", tile,"_harm_lm2_loess_03_scaled.envi"))
  print("loaded")
  df_metrics_tile  <- extract(b_metrics, pnt_JD, cellnumbers=T, df=T)
  df_metrics_tile <- cbind(tile=rep(tile, nrow(df_metrics_tile)), df_metrics_tile)
  df_metrics_tile  <- na.exclude(df_metrics_tile)
  df_covs_tile <- df_metrics_tile
  print(names(df_covs_tile))
  print( nrow(df_covs_tile))
  df_covs_tile
}
```

Merged dataframe on ref data and cell values

```{r,eval=T}
# JD
df_ref_JD <- pnt_JD@data[df_covs_JD$ID,]
df_model_JD <- cbind(Code=df_ref_JD$Code, df_covs_JD)
# good to save this!
```

Clear NAs

```{r,eval=T}
# exclude some classes
df_model_JD <- subset(df_model_JD, Code <= 5)
# exclude NAs
cc <- complete.cases(df_model_JD)
table(df_model_JD$Code[cc])
names(df_model_JD)
# assign names to classes
df_model_JD$LC[df_model_JD$Code == 1] <- "Forest"
df_model_JD$LC[df_model_JD$Code == 2] <- "Shrubland"
df_model_JD$LC[df_model_JD$Code == 3] <- "Grassland"
df_model_JD$LC[df_model_JD$Code == 4] <- "Cropland"
df_model_JD$LC[df_model_JD$Code == 5] <- "Bare"

# convert to factor
df_model_JD$LC <- as.factor(df_model_JD$LC)
# only complete cases
table(df_model_JD$LC[cc])
glimpse(df_model_JD[cc, -(2:4)])
```

"Ranger is a fast implementation of random forest (Breiman 2001) or recursive partitioning, particularly suited for high dimensional data. Classification, regression, probability estimation and survival forests are supported. Classification and regression forests are implemented as in the original Random Forest (Breiman 2001), survival forests as in Random Survival Forests (Ishwaran et al. 2008). For probability estimation forests see Malley et al. (2012)." [source](https://github.com/imbs-hl/ranger)

```{r,eval=T}
# run ranger on Land cover classification
cat("------- ranger ---------")
ra_JD <- ranger(LC ~ ., df_model_JD[cc, -(1:4)], num.trees=500, write.forest=T,
                  probability = F, num.threads=10, verbose=T, importance = "impurity")
# save the ranger model
dir.create('data',showWarnings = F)
dir.create('data/models',showWarnings = F)
saveRDS(ra_JD, "data/models/ra_JD_merge5_x16.rds")
print(ra_JD)
```

dont evaluate this section

```{r, eval=T, message=FALSE}
#### ------------- predict -------------------------------------------
dir.create('rsdata/probav/results',showWarnings = F)
# using ranger
model <- readRDS("data/models/ra_JD_merge5_x16.rds")
tiles <- c("X17Y06")

for (tile in tiles){
  print(paste0("--------------", tile, "-------------------"))
  # -------------------------------data----------------------------------#
  b_metrics <- b_metrics
  print("---predict--------------")
  
  out_name <- paste0(getwd(), "/rsdata/probav/results/pred_JD_", tile,  ".tif")

  pred_JD <- mcPredictSpatial(model, b_metrics, b_clumps=NULL, df_clumps = NULL, type='response',
                                mc.cores = 5, ranger_threads = 1, minrows = 12, logfile = logfile,
                                datatype ="INT1U", of ="GTiff", out_name = out_name)
  
  print(pred_JD)
}
```

Check output!

```{r,eval=T}
r <- raster(paste0(getwd(), "/rsdata/probav/results/pred_JD_", tile,  ".tif"))
r <- as.factor(r)
rat <- levels(r)[[1]]
rat[["landcover"]] <- levels(ra_JD$predictions) # double check this
levels(r) <- rat

# add/change colours if you have more legend items
my_col = c('yellow','darkgreen','lightgreen')
## Plot
levelplot(r, col.regions=my_col, xlab="", ylab="")
```

[creative commens](https://creativecommons.org/licenses/by-sa/4.0/) ![CC logo](https://i.creativecommons.org/l/by-sa/4.0/80x15.png)

